{
  "best_metric": 0.6979219533340298,
  "best_model_checkpoint": "models\\sentiment_distilbert\\hf_outputs\\checkpoint-5702",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 11404,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008768853034023149,
      "grad_norm": 2.78609037399292,
      "learning_rate": 1.994154097977318e-05,
      "loss": 1.0106,
      "step": 50
    },
    {
      "epoch": 0.017537706068046298,
      "grad_norm": 4.405348777770996,
      "learning_rate": 1.988308195954636e-05,
      "loss": 0.9605,
      "step": 100
    },
    {
      "epoch": 0.02630655910206945,
      "grad_norm": 5.132295608520508,
      "learning_rate": 1.9825792119724076e-05,
      "loss": 0.863,
      "step": 150
    },
    {
      "epoch": 0.035075412136092596,
      "grad_norm": 15.297231674194336,
      "learning_rate": 1.9767333099497254e-05,
      "loss": 0.7744,
      "step": 200
    },
    {
      "epoch": 0.04384426517011575,
      "grad_norm": 8.914892196655273,
      "learning_rate": 1.971004325967497e-05,
      "loss": 0.8002,
      "step": 250
    },
    {
      "epoch": 0.0526131182041389,
      "grad_norm": 6.1103620529174805,
      "learning_rate": 1.965158423944815e-05,
      "loss": 0.7363,
      "step": 300
    },
    {
      "epoch": 0.061381971238162046,
      "grad_norm": 10.307082176208496,
      "learning_rate": 1.9593125219221328e-05,
      "loss": 0.716,
      "step": 350
    },
    {
      "epoch": 0.07015082427218519,
      "grad_norm": 6.216363906860352,
      "learning_rate": 1.953466619899451e-05,
      "loss": 0.8027,
      "step": 400
    },
    {
      "epoch": 0.07891967730620834,
      "grad_norm": 7.623715400695801,
      "learning_rate": 1.9476207178767687e-05,
      "loss": 0.7357,
      "step": 450
    },
    {
      "epoch": 0.0876885303402315,
      "grad_norm": 5.970484256744385,
      "learning_rate": 1.9417748158540864e-05,
      "loss": 0.7109,
      "step": 500
    },
    {
      "epoch": 0.09645738337425465,
      "grad_norm": 8.468155860900879,
      "learning_rate": 1.9359289138314042e-05,
      "loss": 0.7474,
      "step": 550
    },
    {
      "epoch": 0.1052262364082778,
      "grad_norm": 5.409418106079102,
      "learning_rate": 1.930083011808722e-05,
      "loss": 0.6828,
      "step": 600
    },
    {
      "epoch": 0.11399508944230095,
      "grad_norm": 9.056489944458008,
      "learning_rate": 1.92423710978604e-05,
      "loss": 0.7383,
      "step": 650
    },
    {
      "epoch": 0.12276394247632409,
      "grad_norm": 7.209068298339844,
      "learning_rate": 1.918391207763358e-05,
      "loss": 0.7533,
      "step": 700
    },
    {
      "epoch": 0.13153279551034724,
      "grad_norm": 11.150015830993652,
      "learning_rate": 1.912545305740676e-05,
      "loss": 0.7387,
      "step": 750
    },
    {
      "epoch": 0.14030164854437038,
      "grad_norm": 11.48244857788086,
      "learning_rate": 1.9066994037179938e-05,
      "loss": 0.736,
      "step": 800
    },
    {
      "epoch": 0.14907050157839355,
      "grad_norm": 10.70394229888916,
      "learning_rate": 1.900853501695312e-05,
      "loss": 0.6696,
      "step": 850
    },
    {
      "epoch": 0.1578393546124167,
      "grad_norm": 9.964875221252441,
      "learning_rate": 1.8950075996726297e-05,
      "loss": 0.6694,
      "step": 900
    },
    {
      "epoch": 0.16660820764643985,
      "grad_norm": 6.128086090087891,
      "learning_rate": 1.8891616976499478e-05,
      "loss": 0.7266,
      "step": 950
    },
    {
      "epoch": 0.175377060680463,
      "grad_norm": 8.196494102478027,
      "learning_rate": 1.8833157956272656e-05,
      "loss": 0.7679,
      "step": 1000
    },
    {
      "epoch": 0.18414591371448616,
      "grad_norm": 3.785667896270752,
      "learning_rate": 1.8774698936045833e-05,
      "loss": 0.7697,
      "step": 1050
    },
    {
      "epoch": 0.1929147667485093,
      "grad_norm": 6.847892761230469,
      "learning_rate": 1.871623991581901e-05,
      "loss": 0.6864,
      "step": 1100
    },
    {
      "epoch": 0.20168361978253244,
      "grad_norm": 11.189318656921387,
      "learning_rate": 1.8657780895592192e-05,
      "loss": 0.6862,
      "step": 1150
    },
    {
      "epoch": 0.2104524728165556,
      "grad_norm": 12.154191017150879,
      "learning_rate": 1.859932187536537e-05,
      "loss": 0.7452,
      "step": 1200
    },
    {
      "epoch": 0.21922132585057874,
      "grad_norm": 9.27708625793457,
      "learning_rate": 1.8540862855138548e-05,
      "loss": 0.7231,
      "step": 1250
    },
    {
      "epoch": 0.2279901788846019,
      "grad_norm": 11.412940979003906,
      "learning_rate": 1.848240383491173e-05,
      "loss": 0.739,
      "step": 1300
    },
    {
      "epoch": 0.23675903191862505,
      "grad_norm": 7.650617599487305,
      "learning_rate": 1.8423944814684907e-05,
      "loss": 0.689,
      "step": 1350
    },
    {
      "epoch": 0.24552788495264818,
      "grad_norm": 10.387537956237793,
      "learning_rate": 1.8365485794458088e-05,
      "loss": 0.7143,
      "step": 1400
    },
    {
      "epoch": 0.25429673798667135,
      "grad_norm": 5.376278400421143,
      "learning_rate": 1.8307026774231266e-05,
      "loss": 0.6391,
      "step": 1450
    },
    {
      "epoch": 0.2630655910206945,
      "grad_norm": 6.430680751800537,
      "learning_rate": 1.8248567754004443e-05,
      "loss": 0.7101,
      "step": 1500
    },
    {
      "epoch": 0.2718344440547176,
      "grad_norm": 11.026700973510742,
      "learning_rate": 1.819010873377762e-05,
      "loss": 0.6436,
      "step": 1550
    },
    {
      "epoch": 0.28060329708874077,
      "grad_norm": 19.28641700744629,
      "learning_rate": 1.8131649713550802e-05,
      "loss": 0.6819,
      "step": 1600
    },
    {
      "epoch": 0.28937215012276396,
      "grad_norm": 6.342319965362549,
      "learning_rate": 1.807319069332398e-05,
      "loss": 0.6486,
      "step": 1650
    },
    {
      "epoch": 0.2981410031567871,
      "grad_norm": 6.3827972412109375,
      "learning_rate": 1.801473167309716e-05,
      "loss": 0.6929,
      "step": 1700
    },
    {
      "epoch": 0.30690985619081024,
      "grad_norm": 6.503917217254639,
      "learning_rate": 1.795627265287034e-05,
      "loss": 0.7055,
      "step": 1750
    },
    {
      "epoch": 0.3156787092248334,
      "grad_norm": 7.862295627593994,
      "learning_rate": 1.789781363264352e-05,
      "loss": 0.624,
      "step": 1800
    },
    {
      "epoch": 0.3244475622588565,
      "grad_norm": 10.600494384765625,
      "learning_rate": 1.7839354612416698e-05,
      "loss": 0.6908,
      "step": 1850
    },
    {
      "epoch": 0.3332164152928797,
      "grad_norm": 13.644340515136719,
      "learning_rate": 1.778089559218988e-05,
      "loss": 0.6094,
      "step": 1900
    },
    {
      "epoch": 0.34198526832690285,
      "grad_norm": 6.878085136413574,
      "learning_rate": 1.7722436571963057e-05,
      "loss": 0.6287,
      "step": 1950
    },
    {
      "epoch": 0.350754121360926,
      "grad_norm": 8.99931812286377,
      "learning_rate": 1.7663977551736234e-05,
      "loss": 0.7207,
      "step": 2000
    },
    {
      "epoch": 0.3595229743949491,
      "grad_norm": 7.750284194946289,
      "learning_rate": 1.7605518531509412e-05,
      "loss": 0.6799,
      "step": 2050
    },
    {
      "epoch": 0.3682918274289723,
      "grad_norm": 11.2012939453125,
      "learning_rate": 1.754705951128259e-05,
      "loss": 0.6887,
      "step": 2100
    },
    {
      "epoch": 0.37706068046299546,
      "grad_norm": 12.072850227355957,
      "learning_rate": 1.748860049105577e-05,
      "loss": 0.6388,
      "step": 2150
    },
    {
      "epoch": 0.3858295334970186,
      "grad_norm": 9.421682357788086,
      "learning_rate": 1.743014147082895e-05,
      "loss": 0.6253,
      "step": 2200
    },
    {
      "epoch": 0.39459838653104173,
      "grad_norm": 15.51858139038086,
      "learning_rate": 1.737168245060213e-05,
      "loss": 0.6344,
      "step": 2250
    },
    {
      "epoch": 0.4033672395650649,
      "grad_norm": 8.501811981201172,
      "learning_rate": 1.7313223430375308e-05,
      "loss": 0.7072,
      "step": 2300
    },
    {
      "epoch": 0.41213609259908807,
      "grad_norm": 9.413457870483398,
      "learning_rate": 1.725476441014849e-05,
      "loss": 0.6721,
      "step": 2350
    },
    {
      "epoch": 0.4209049456331112,
      "grad_norm": 12.418612480163574,
      "learning_rate": 1.71974745703262e-05,
      "loss": 0.6842,
      "step": 2400
    },
    {
      "epoch": 0.42967379866713434,
      "grad_norm": 6.502837657928467,
      "learning_rate": 1.7139015550099382e-05,
      "loss": 0.622,
      "step": 2450
    },
    {
      "epoch": 0.4384426517011575,
      "grad_norm": 6.329768180847168,
      "learning_rate": 1.708055652987256e-05,
      "loss": 0.7045,
      "step": 2500
    },
    {
      "epoch": 0.4472115047351806,
      "grad_norm": 6.963382244110107,
      "learning_rate": 1.702209750964574e-05,
      "loss": 0.67,
      "step": 2550
    },
    {
      "epoch": 0.4559803577692038,
      "grad_norm": 4.712369918823242,
      "learning_rate": 1.696363848941892e-05,
      "loss": 0.6054,
      "step": 2600
    },
    {
      "epoch": 0.46474921080322695,
      "grad_norm": 9.84788990020752,
      "learning_rate": 1.6905179469192096e-05,
      "loss": 0.7212,
      "step": 2650
    },
    {
      "epoch": 0.4735180638372501,
      "grad_norm": 5.696516036987305,
      "learning_rate": 1.6846720448965278e-05,
      "loss": 0.6353,
      "step": 2700
    },
    {
      "epoch": 0.48228691687127323,
      "grad_norm": 7.9802327156066895,
      "learning_rate": 1.6788261428738455e-05,
      "loss": 0.7168,
      "step": 2750
    },
    {
      "epoch": 0.49105576990529637,
      "grad_norm": 11.6334228515625,
      "learning_rate": 1.6729802408511636e-05,
      "loss": 0.6892,
      "step": 2800
    },
    {
      "epoch": 0.49982462293931956,
      "grad_norm": 6.101984977722168,
      "learning_rate": 1.6671343388284814e-05,
      "loss": 0.5988,
      "step": 2850
    },
    {
      "epoch": 0.5085934759733427,
      "grad_norm": 13.859321594238281,
      "learning_rate": 1.6612884368057992e-05,
      "loss": 0.691,
      "step": 2900
    },
    {
      "epoch": 0.5173623290073658,
      "grad_norm": 9.972716331481934,
      "learning_rate": 1.655442534783117e-05,
      "loss": 0.6425,
      "step": 2950
    },
    {
      "epoch": 0.526131182041389,
      "grad_norm": 7.131099700927734,
      "learning_rate": 1.649596632760435e-05,
      "loss": 0.645,
      "step": 3000
    },
    {
      "epoch": 0.5349000350754122,
      "grad_norm": 9.994009017944336,
      "learning_rate": 1.643750730737753e-05,
      "loss": 0.5886,
      "step": 3050
    },
    {
      "epoch": 0.5436688881094353,
      "grad_norm": 5.116741180419922,
      "learning_rate": 1.637904828715071e-05,
      "loss": 0.7059,
      "step": 3100
    },
    {
      "epoch": 0.5524377411434584,
      "grad_norm": 6.788241863250732,
      "learning_rate": 1.6320589266923888e-05,
      "loss": 0.6098,
      "step": 3150
    },
    {
      "epoch": 0.5612065941774815,
      "grad_norm": 9.483665466308594,
      "learning_rate": 1.626213024669707e-05,
      "loss": 0.7098,
      "step": 3200
    },
    {
      "epoch": 0.5699754472115047,
      "grad_norm": 8.083919525146484,
      "learning_rate": 1.6203671226470246e-05,
      "loss": 0.653,
      "step": 3250
    },
    {
      "epoch": 0.5787443002455279,
      "grad_norm": 7.0439958572387695,
      "learning_rate": 1.6145212206243424e-05,
      "loss": 0.6161,
      "step": 3300
    },
    {
      "epoch": 0.587513153279551,
      "grad_norm": 5.529977798461914,
      "learning_rate": 1.6086753186016605e-05,
      "loss": 0.6665,
      "step": 3350
    },
    {
      "epoch": 0.5962820063135742,
      "grad_norm": 10.408880233764648,
      "learning_rate": 1.6028294165789783e-05,
      "loss": 0.6141,
      "step": 3400
    },
    {
      "epoch": 0.6050508593475973,
      "grad_norm": 10.851572036743164,
      "learning_rate": 1.596983514556296e-05,
      "loss": 0.5766,
      "step": 3450
    },
    {
      "epoch": 0.6138197123816205,
      "grad_norm": 8.411799430847168,
      "learning_rate": 1.591137612533614e-05,
      "loss": 0.7359,
      "step": 3500
    },
    {
      "epoch": 0.6225885654156437,
      "grad_norm": 6.488907337188721,
      "learning_rate": 1.585291710510932e-05,
      "loss": 0.6549,
      "step": 3550
    },
    {
      "epoch": 0.6313574184496668,
      "grad_norm": 10.763432502746582,
      "learning_rate": 1.5794458084882498e-05,
      "loss": 0.6894,
      "step": 3600
    },
    {
      "epoch": 0.64012627148369,
      "grad_norm": 9.205035209655762,
      "learning_rate": 1.573599906465568e-05,
      "loss": 0.632,
      "step": 3650
    },
    {
      "epoch": 0.648895124517713,
      "grad_norm": 12.431971549987793,
      "learning_rate": 1.5677540044428856e-05,
      "loss": 0.6312,
      "step": 3700
    },
    {
      "epoch": 0.6576639775517362,
      "grad_norm": 13.151571273803711,
      "learning_rate": 1.5619081024202038e-05,
      "loss": 0.6654,
      "step": 3750
    },
    {
      "epoch": 0.6664328305857594,
      "grad_norm": 8.816189765930176,
      "learning_rate": 1.5560622003975215e-05,
      "loss": 0.6495,
      "step": 3800
    },
    {
      "epoch": 0.6752016836197825,
      "grad_norm": 7.93595552444458,
      "learning_rate": 1.5502162983748393e-05,
      "loss": 0.613,
      "step": 3850
    },
    {
      "epoch": 0.6839705366538057,
      "grad_norm": 7.239200592041016,
      "learning_rate": 1.544370396352157e-05,
      "loss": 0.674,
      "step": 3900
    },
    {
      "epoch": 0.6927393896878288,
      "grad_norm": 7.882876873016357,
      "learning_rate": 1.5385244943294752e-05,
      "loss": 0.6345,
      "step": 3950
    },
    {
      "epoch": 0.701508242721852,
      "grad_norm": 6.4954833984375,
      "learning_rate": 1.532678592306793e-05,
      "loss": 0.7186,
      "step": 4000
    },
    {
      "epoch": 0.7102770957558752,
      "grad_norm": 4.600713729858398,
      "learning_rate": 1.5268326902841107e-05,
      "loss": 0.6289,
      "step": 4050
    },
    {
      "epoch": 0.7190459487898982,
      "grad_norm": 10.321553230285645,
      "learning_rate": 1.5209867882614289e-05,
      "loss": 0.5809,
      "step": 4100
    },
    {
      "epoch": 0.7278148018239214,
      "grad_norm": 9.643561363220215,
      "learning_rate": 1.5151408862387466e-05,
      "loss": 0.6228,
      "step": 4150
    },
    {
      "epoch": 0.7365836548579446,
      "grad_norm": 7.35713529586792,
      "learning_rate": 1.5092949842160648e-05,
      "loss": 0.6373,
      "step": 4200
    },
    {
      "epoch": 0.7453525078919677,
      "grad_norm": 4.078065395355225,
      "learning_rate": 1.5034490821933825e-05,
      "loss": 0.6535,
      "step": 4250
    },
    {
      "epoch": 0.7541213609259909,
      "grad_norm": 6.547789096832275,
      "learning_rate": 1.4976031801707005e-05,
      "loss": 0.644,
      "step": 4300
    },
    {
      "epoch": 0.762890213960014,
      "grad_norm": 3.811894178390503,
      "learning_rate": 1.4917572781480183e-05,
      "loss": 0.6566,
      "step": 4350
    },
    {
      "epoch": 0.7716590669940372,
      "grad_norm": 7.036433219909668,
      "learning_rate": 1.4859113761253364e-05,
      "loss": 0.6045,
      "step": 4400
    },
    {
      "epoch": 0.7804279200280604,
      "grad_norm": 9.200966835021973,
      "learning_rate": 1.4800654741026541e-05,
      "loss": 0.6145,
      "step": 4450
    },
    {
      "epoch": 0.7891967730620835,
      "grad_norm": 8.785243034362793,
      "learning_rate": 1.4742195720799721e-05,
      "loss": 0.6161,
      "step": 4500
    },
    {
      "epoch": 0.7979656260961067,
      "grad_norm": 13.086260795593262,
      "learning_rate": 1.4683736700572899e-05,
      "loss": 0.6052,
      "step": 4550
    },
    {
      "epoch": 0.8067344791301297,
      "grad_norm": 6.894944190979004,
      "learning_rate": 1.4626446860750616e-05,
      "loss": 0.63,
      "step": 4600
    },
    {
      "epoch": 0.8155033321641529,
      "grad_norm": 7.988692760467529,
      "learning_rate": 1.4567987840523793e-05,
      "loss": 0.5769,
      "step": 4650
    },
    {
      "epoch": 0.8242721851981761,
      "grad_norm": 9.271968841552734,
      "learning_rate": 1.4509528820296971e-05,
      "loss": 0.6251,
      "step": 4700
    },
    {
      "epoch": 0.8330410382321992,
      "grad_norm": 13.432433128356934,
      "learning_rate": 1.4451069800070152e-05,
      "loss": 0.6311,
      "step": 4750
    },
    {
      "epoch": 0.8418098912662224,
      "grad_norm": 19.611053466796875,
      "learning_rate": 1.439261077984333e-05,
      "loss": 0.6351,
      "step": 4800
    },
    {
      "epoch": 0.8505787443002455,
      "grad_norm": 17.896814346313477,
      "learning_rate": 1.433415175961651e-05,
      "loss": 0.5976,
      "step": 4850
    },
    {
      "epoch": 0.8593475973342687,
      "grad_norm": 9.070780754089355,
      "learning_rate": 1.4275692739389689e-05,
      "loss": 0.5921,
      "step": 4900
    },
    {
      "epoch": 0.8681164503682919,
      "grad_norm": 5.377007961273193,
      "learning_rate": 1.4217233719162868e-05,
      "loss": 0.6235,
      "step": 4950
    },
    {
      "epoch": 0.876885303402315,
      "grad_norm": 7.689847946166992,
      "learning_rate": 1.4158774698936046e-05,
      "loss": 0.6526,
      "step": 5000
    },
    {
      "epoch": 0.8856541564363382,
      "grad_norm": 8.640979766845703,
      "learning_rate": 1.4100315678709227e-05,
      "loss": 0.6752,
      "step": 5050
    },
    {
      "epoch": 0.8944230094703612,
      "grad_norm": 7.424570083618164,
      "learning_rate": 1.4041856658482405e-05,
      "loss": 0.6661,
      "step": 5100
    },
    {
      "epoch": 0.9031918625043844,
      "grad_norm": 20.304109573364258,
      "learning_rate": 1.3983397638255584e-05,
      "loss": 0.6471,
      "step": 5150
    },
    {
      "epoch": 0.9119607155384076,
      "grad_norm": 7.652063846588135,
      "learning_rate": 1.3924938618028762e-05,
      "loss": 0.635,
      "step": 5200
    },
    {
      "epoch": 0.9207295685724307,
      "grad_norm": 5.61309289932251,
      "learning_rate": 1.3866479597801943e-05,
      "loss": 0.6494,
      "step": 5250
    },
    {
      "epoch": 0.9294984216064539,
      "grad_norm": 11.51627254486084,
      "learning_rate": 1.3808020577575121e-05,
      "loss": 0.6111,
      "step": 5300
    },
    {
      "epoch": 0.938267274640477,
      "grad_norm": 8.385472297668457,
      "learning_rate": 1.37495615573483e-05,
      "loss": 0.6093,
      "step": 5350
    },
    {
      "epoch": 0.9470361276745002,
      "grad_norm": 7.989416599273682,
      "learning_rate": 1.3691102537121478e-05,
      "loss": 0.6081,
      "step": 5400
    },
    {
      "epoch": 0.9558049807085234,
      "grad_norm": 5.929741382598877,
      "learning_rate": 1.3632643516894656e-05,
      "loss": 0.5634,
      "step": 5450
    },
    {
      "epoch": 0.9645738337425465,
      "grad_norm": 8.158644676208496,
      "learning_rate": 1.3574184496667837e-05,
      "loss": 0.5934,
      "step": 5500
    },
    {
      "epoch": 0.9733426867765697,
      "grad_norm": 5.380097389221191,
      "learning_rate": 1.3515725476441015e-05,
      "loss": 0.6121,
      "step": 5550
    },
    {
      "epoch": 0.9821115398105927,
      "grad_norm": 12.424233436584473,
      "learning_rate": 1.3457266456214194e-05,
      "loss": 0.6257,
      "step": 5600
    },
    {
      "epoch": 0.9908803928446159,
      "grad_norm": 6.425619125366211,
      "learning_rate": 1.3398807435987374e-05,
      "loss": 0.595,
      "step": 5650
    },
    {
      "epoch": 0.9996492458786391,
      "grad_norm": 6.724080562591553,
      "learning_rate": 1.3340348415760553e-05,
      "loss": 0.642,
      "step": 5700
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.6979219533340298,
      "eval_loss": 0.6643175482749939,
      "eval_runtime": 160.8306,
      "eval_samples_per_second": 76.378,
      "eval_steps_per_second": 9.55,
      "step": 5702
    },
    {
      "epoch": 1.0084180989126623,
      "grad_norm": 13.185784339904785,
      "learning_rate": 1.3281889395533731e-05,
      "loss": 0.5165,
      "step": 5750
    },
    {
      "epoch": 1.0171869519466854,
      "grad_norm": 3.195570468902588,
      "learning_rate": 1.3223430375306912e-05,
      "loss": 0.5,
      "step": 5800
    },
    {
      "epoch": 1.0259558049807085,
      "grad_norm": 7.197144031524658,
      "learning_rate": 1.316497135508009e-05,
      "loss": 0.5382,
      "step": 5850
    },
    {
      "epoch": 1.0347246580147316,
      "grad_norm": 4.68015193939209,
      "learning_rate": 1.310651233485327e-05,
      "loss": 0.5078,
      "step": 5900
    },
    {
      "epoch": 1.0434935110487549,
      "grad_norm": 10.591974258422852,
      "learning_rate": 1.3048053314626447e-05,
      "loss": 0.4374,
      "step": 5950
    },
    {
      "epoch": 1.052262364082778,
      "grad_norm": 9.651126861572266,
      "learning_rate": 1.2989594294399628e-05,
      "loss": 0.4784,
      "step": 6000
    },
    {
      "epoch": 1.061031217116801,
      "grad_norm": 6.314791202545166,
      "learning_rate": 1.2931135274172806e-05,
      "loss": 0.4718,
      "step": 6050
    },
    {
      "epoch": 1.0698000701508243,
      "grad_norm": 5.393407821655273,
      "learning_rate": 1.2872676253945986e-05,
      "loss": 0.5149,
      "step": 6100
    },
    {
      "epoch": 1.0785689231848474,
      "grad_norm": 15.326130867004395,
      "learning_rate": 1.2814217233719163e-05,
      "loss": 0.5103,
      "step": 6150
    },
    {
      "epoch": 1.0873377762188705,
      "grad_norm": 7.9577789306640625,
      "learning_rate": 1.2755758213492341e-05,
      "loss": 0.5296,
      "step": 6200
    },
    {
      "epoch": 1.0961066292528938,
      "grad_norm": 9.567963600158691,
      "learning_rate": 1.2697299193265522e-05,
      "loss": 0.4373,
      "step": 6250
    },
    {
      "epoch": 1.104875482286917,
      "grad_norm": 5.636972904205322,
      "learning_rate": 1.26388401730387e-05,
      "loss": 0.4621,
      "step": 6300
    },
    {
      "epoch": 1.11364433532094,
      "grad_norm": 15.910542488098145,
      "learning_rate": 1.258038115281188e-05,
      "loss": 0.4807,
      "step": 6350
    },
    {
      "epoch": 1.122413188354963,
      "grad_norm": 8.770207405090332,
      "learning_rate": 1.2521922132585059e-05,
      "loss": 0.5224,
      "step": 6400
    },
    {
      "epoch": 1.1311820413889864,
      "grad_norm": 9.428196907043457,
      "learning_rate": 1.2463463112358238e-05,
      "loss": 0.4878,
      "step": 6450
    },
    {
      "epoch": 1.1399508944230095,
      "grad_norm": 14.291045188903809,
      "learning_rate": 1.2405004092131416e-05,
      "loss": 0.5004,
      "step": 6500
    },
    {
      "epoch": 1.1487197474570325,
      "grad_norm": 8.06965160369873,
      "learning_rate": 1.2346545071904597e-05,
      "loss": 0.5179,
      "step": 6550
    },
    {
      "epoch": 1.1574886004910558,
      "grad_norm": 3.1708855628967285,
      "learning_rate": 1.2288086051677775e-05,
      "loss": 0.5039,
      "step": 6600
    },
    {
      "epoch": 1.166257453525079,
      "grad_norm": 6.842585563659668,
      "learning_rate": 1.2230796211855492e-05,
      "loss": 0.5786,
      "step": 6650
    },
    {
      "epoch": 1.175026306559102,
      "grad_norm": 6.977348804473877,
      "learning_rate": 1.217233719162867e-05,
      "loss": 0.4831,
      "step": 6700
    },
    {
      "epoch": 1.1837951595931253,
      "grad_norm": 14.591277122497559,
      "learning_rate": 1.2113878171401848e-05,
      "loss": 0.5341,
      "step": 6750
    },
    {
      "epoch": 1.1925640126271484,
      "grad_norm": 16.683696746826172,
      "learning_rate": 1.2055419151175027e-05,
      "loss": 0.5117,
      "step": 6800
    },
    {
      "epoch": 1.2013328656611715,
      "grad_norm": 8.87572193145752,
      "learning_rate": 1.1996960130948205e-05,
      "loss": 0.5037,
      "step": 6850
    },
    {
      "epoch": 1.2101017186951948,
      "grad_norm": 16.402578353881836,
      "learning_rate": 1.1938501110721386e-05,
      "loss": 0.5568,
      "step": 6900
    },
    {
      "epoch": 1.2188705717292179,
      "grad_norm": 7.4139323234558105,
      "learning_rate": 1.1880042090494564e-05,
      "loss": 0.5956,
      "step": 6950
    },
    {
      "epoch": 1.227639424763241,
      "grad_norm": 9.580617904663086,
      "learning_rate": 1.1821583070267743e-05,
      "loss": 0.5179,
      "step": 7000
    },
    {
      "epoch": 1.236408277797264,
      "grad_norm": 7.460948467254639,
      "learning_rate": 1.1763124050040921e-05,
      "loss": 0.4882,
      "step": 7050
    },
    {
      "epoch": 1.2451771308312873,
      "grad_norm": 10.990803718566895,
      "learning_rate": 1.1704665029814102e-05,
      "loss": 0.5291,
      "step": 7100
    },
    {
      "epoch": 1.2539459838653104,
      "grad_norm": 6.368955135345459,
      "learning_rate": 1.164620600958728e-05,
      "loss": 0.5424,
      "step": 7150
    },
    {
      "epoch": 1.2627148368993335,
      "grad_norm": 11.358160972595215,
      "learning_rate": 1.158774698936046e-05,
      "loss": 0.4601,
      "step": 7200
    },
    {
      "epoch": 1.2714836899333566,
      "grad_norm": 26.179758071899414,
      "learning_rate": 1.1529287969133639e-05,
      "loss": 0.5155,
      "step": 7250
    },
    {
      "epoch": 1.28025254296738,
      "grad_norm": 10.728906631469727,
      "learning_rate": 1.1470828948906818e-05,
      "loss": 0.4496,
      "step": 7300
    },
    {
      "epoch": 1.289021396001403,
      "grad_norm": 12.006559371948242,
      "learning_rate": 1.1412369928679996e-05,
      "loss": 0.4743,
      "step": 7350
    },
    {
      "epoch": 1.297790249035426,
      "grad_norm": 15.346386909484863,
      "learning_rate": 1.1353910908453177e-05,
      "loss": 0.5066,
      "step": 7400
    },
    {
      "epoch": 1.3065591020694494,
      "grad_norm": 17.93372344970703,
      "learning_rate": 1.1295451888226355e-05,
      "loss": 0.4752,
      "step": 7450
    },
    {
      "epoch": 1.3153279551034724,
      "grad_norm": 6.520020961761475,
      "learning_rate": 1.1236992867999533e-05,
      "loss": 0.4276,
      "step": 7500
    },
    {
      "epoch": 1.3240968081374955,
      "grad_norm": 18.699878692626953,
      "learning_rate": 1.1178533847772712e-05,
      "loss": 0.4897,
      "step": 7550
    },
    {
      "epoch": 1.3328656611715188,
      "grad_norm": 12.651447296142578,
      "learning_rate": 1.112007482754589e-05,
      "loss": 0.5375,
      "step": 7600
    },
    {
      "epoch": 1.341634514205542,
      "grad_norm": 14.601369857788086,
      "learning_rate": 1.1061615807319071e-05,
      "loss": 0.5583,
      "step": 7650
    },
    {
      "epoch": 1.350403367239565,
      "grad_norm": 7.107217788696289,
      "learning_rate": 1.1003156787092249e-05,
      "loss": 0.4544,
      "step": 7700
    },
    {
      "epoch": 1.3591722202735883,
      "grad_norm": 4.514739036560059,
      "learning_rate": 1.0944697766865428e-05,
      "loss": 0.4539,
      "step": 7750
    },
    {
      "epoch": 1.3679410733076114,
      "grad_norm": 5.166164398193359,
      "learning_rate": 1.0886238746638606e-05,
      "loss": 0.4327,
      "step": 7800
    },
    {
      "epoch": 1.3767099263416345,
      "grad_norm": 14.141812324523926,
      "learning_rate": 1.0827779726411787e-05,
      "loss": 0.547,
      "step": 7850
    },
    {
      "epoch": 1.3854787793756578,
      "grad_norm": 8.085691452026367,
      "learning_rate": 1.0769320706184965e-05,
      "loss": 0.4753,
      "step": 7900
    },
    {
      "epoch": 1.3942476324096809,
      "grad_norm": 3.319336414337158,
      "learning_rate": 1.0710861685958144e-05,
      "loss": 0.4447,
      "step": 7950
    },
    {
      "epoch": 1.403016485443704,
      "grad_norm": 11.340115547180176,
      "learning_rate": 1.0652402665731324e-05,
      "loss": 0.507,
      "step": 8000
    },
    {
      "epoch": 1.4117853384777272,
      "grad_norm": 17.51145362854004,
      "learning_rate": 1.0593943645504503e-05,
      "loss": 0.4784,
      "step": 8050
    },
    {
      "epoch": 1.4205541915117503,
      "grad_norm": 6.903362274169922,
      "learning_rate": 1.0535484625277681e-05,
      "loss": 0.4986,
      "step": 8100
    },
    {
      "epoch": 1.4293230445457734,
      "grad_norm": 14.143353462219238,
      "learning_rate": 1.047702560505086e-05,
      "loss": 0.4586,
      "step": 8150
    },
    {
      "epoch": 1.4380918975797965,
      "grad_norm": 15.857844352722168,
      "learning_rate": 1.041856658482404e-05,
      "loss": 0.495,
      "step": 8200
    },
    {
      "epoch": 1.4468607506138196,
      "grad_norm": 6.713830947875977,
      "learning_rate": 1.0360107564597218e-05,
      "loss": 0.469,
      "step": 8250
    },
    {
      "epoch": 1.4556296036478429,
      "grad_norm": 9.731610298156738,
      "learning_rate": 1.0301648544370397e-05,
      "loss": 0.4279,
      "step": 8300
    },
    {
      "epoch": 1.464398456681866,
      "grad_norm": 9.63312816619873,
      "learning_rate": 1.0243189524143575e-05,
      "loss": 0.4765,
      "step": 8350
    },
    {
      "epoch": 1.473167309715889,
      "grad_norm": 6.465417861938477,
      "learning_rate": 1.0184730503916756e-05,
      "loss": 0.4875,
      "step": 8400
    },
    {
      "epoch": 1.4819361627499124,
      "grad_norm": 9.533226013183594,
      "learning_rate": 1.0126271483689934e-05,
      "loss": 0.4628,
      "step": 8450
    },
    {
      "epoch": 1.4907050157839354,
      "grad_norm": 10.522842407226562,
      "learning_rate": 1.0067812463463113e-05,
      "loss": 0.5376,
      "step": 8500
    },
    {
      "epoch": 1.4994738688179585,
      "grad_norm": 12.263348579406738,
      "learning_rate": 1.0009353443236291e-05,
      "loss": 0.475,
      "step": 8550
    },
    {
      "epoch": 1.5082427218519818,
      "grad_norm": 18.457218170166016,
      "learning_rate": 9.950894423009472e-06,
      "loss": 0.475,
      "step": 8600
    },
    {
      "epoch": 1.517011574886005,
      "grad_norm": 7.831368923187256,
      "learning_rate": 9.893604583187186e-06,
      "loss": 0.4246,
      "step": 8650
    },
    {
      "epoch": 1.525780427920028,
      "grad_norm": 3.737313747406006,
      "learning_rate": 9.835145562960365e-06,
      "loss": 0.4889,
      "step": 8700
    },
    {
      "epoch": 1.5345492809540513,
      "grad_norm": 23.667850494384766,
      "learning_rate": 9.776686542733545e-06,
      "loss": 0.4871,
      "step": 8750
    },
    {
      "epoch": 1.5433181339880744,
      "grad_norm": 16.62641143798828,
      "learning_rate": 9.718227522506724e-06,
      "loss": 0.5258,
      "step": 8800
    },
    {
      "epoch": 1.5520869870220975,
      "grad_norm": 7.544626712799072,
      "learning_rate": 9.659768502279903e-06,
      "loss": 0.5326,
      "step": 8850
    },
    {
      "epoch": 1.5608558400561208,
      "grad_norm": 7.36379337310791,
      "learning_rate": 9.601309482053081e-06,
      "loss": 0.4466,
      "step": 8900
    },
    {
      "epoch": 1.5696246930901439,
      "grad_norm": 9.89643669128418,
      "learning_rate": 9.54285046182626e-06,
      "loss": 0.5076,
      "step": 8950
    },
    {
      "epoch": 1.578393546124167,
      "grad_norm": 8.472321510314941,
      "learning_rate": 9.48439144159944e-06,
      "loss": 0.4614,
      "step": 9000
    },
    {
      "epoch": 1.5871623991581902,
      "grad_norm": 12.222833633422852,
      "learning_rate": 9.42593242137262e-06,
      "loss": 0.5095,
      "step": 9050
    },
    {
      "epoch": 1.595931252192213,
      "grad_norm": 14.082836151123047,
      "learning_rate": 9.367473401145797e-06,
      "loss": 0.4332,
      "step": 9100
    },
    {
      "epoch": 1.6047001052262364,
      "grad_norm": 1.836118221282959,
      "learning_rate": 9.309014380918977e-06,
      "loss": 0.4922,
      "step": 9150
    },
    {
      "epoch": 1.6134689582602597,
      "grad_norm": 11.461652755737305,
      "learning_rate": 9.250555360692156e-06,
      "loss": 0.5189,
      "step": 9200
    },
    {
      "epoch": 1.6222378112942826,
      "grad_norm": 7.256875038146973,
      "learning_rate": 9.192096340465334e-06,
      "loss": 0.5479,
      "step": 9250
    },
    {
      "epoch": 1.6310066643283059,
      "grad_norm": 7.556775093078613,
      "learning_rate": 9.133637320238513e-06,
      "loss": 0.4524,
      "step": 9300
    },
    {
      "epoch": 1.639775517362329,
      "grad_norm": 7.143393039703369,
      "learning_rate": 9.075178300011693e-06,
      "loss": 0.4751,
      "step": 9350
    },
    {
      "epoch": 1.648544370396352,
      "grad_norm": 11.834745407104492,
      "learning_rate": 9.01671927978487e-06,
      "loss": 0.501,
      "step": 9400
    },
    {
      "epoch": 1.6573132234303753,
      "grad_norm": 18.979345321655273,
      "learning_rate": 8.95826025955805e-06,
      "loss": 0.431,
      "step": 9450
    },
    {
      "epoch": 1.6660820764643984,
      "grad_norm": 11.046215057373047,
      "learning_rate": 8.89980123933123e-06,
      "loss": 0.4644,
      "step": 9500
    },
    {
      "epoch": 1.6748509294984215,
      "grad_norm": 5.147363185882568,
      "learning_rate": 8.841342219104409e-06,
      "loss": 0.4914,
      "step": 9550
    },
    {
      "epoch": 1.6836197825324448,
      "grad_norm": 14.410489082336426,
      "learning_rate": 8.782883198877587e-06,
      "loss": 0.4741,
      "step": 9600
    },
    {
      "epoch": 1.692388635566468,
      "grad_norm": 6.612221717834473,
      "learning_rate": 8.724424178650766e-06,
      "loss": 0.4885,
      "step": 9650
    },
    {
      "epoch": 1.701157488600491,
      "grad_norm": 6.378579139709473,
      "learning_rate": 8.665965158423946e-06,
      "loss": 0.4327,
      "step": 9700
    },
    {
      "epoch": 1.7099263416345143,
      "grad_norm": 8.7021484375,
      "learning_rate": 8.607506138197125e-06,
      "loss": 0.5205,
      "step": 9750
    },
    {
      "epoch": 1.7186951946685374,
      "grad_norm": 8.619649887084961,
      "learning_rate": 8.549047117970305e-06,
      "loss": 0.4496,
      "step": 9800
    },
    {
      "epoch": 1.7274640477025605,
      "grad_norm": 15.8871488571167,
      "learning_rate": 8.490588097743482e-06,
      "loss": 0.46,
      "step": 9850
    },
    {
      "epoch": 1.7362329007365838,
      "grad_norm": 17.941707611083984,
      "learning_rate": 8.432129077516662e-06,
      "loss": 0.4346,
      "step": 9900
    },
    {
      "epoch": 1.7450017537706068,
      "grad_norm": 2.171929359436035,
      "learning_rate": 8.373670057289841e-06,
      "loss": 0.4653,
      "step": 9950
    },
    {
      "epoch": 1.75377060680463,
      "grad_norm": 23.38855743408203,
      "learning_rate": 8.315211037063019e-06,
      "loss": 0.4871,
      "step": 10000
    },
    {
      "epoch": 1.7625394598386532,
      "grad_norm": 13.622480392456055,
      "learning_rate": 8.256752016836198e-06,
      "loss": 0.4456,
      "step": 10050
    },
    {
      "epoch": 1.771308312872676,
      "grad_norm": 27.93704605102539,
      "learning_rate": 8.199462177013914e-06,
      "loss": 0.5058,
      "step": 10100
    },
    {
      "epoch": 1.7800771659066994,
      "grad_norm": 16.520423889160156,
      "learning_rate": 8.141003156787093e-06,
      "loss": 0.5351,
      "step": 10150
    },
    {
      "epoch": 1.7888460189407227,
      "grad_norm": 18.92119026184082,
      "learning_rate": 8.082544136560273e-06,
      "loss": 0.5089,
      "step": 10200
    },
    {
      "epoch": 1.7976148719747456,
      "grad_norm": 14.71410846710205,
      "learning_rate": 8.02408511633345e-06,
      "loss": 0.4755,
      "step": 10250
    },
    {
      "epoch": 1.8063837250087689,
      "grad_norm": 13.686258316040039,
      "learning_rate": 7.96562609610663e-06,
      "loss": 0.4709,
      "step": 10300
    },
    {
      "epoch": 1.815152578042792,
      "grad_norm": 19.550203323364258,
      "learning_rate": 7.90716707587981e-06,
      "loss": 0.484,
      "step": 10350
    },
    {
      "epoch": 1.823921431076815,
      "grad_norm": 11.703767776489258,
      "learning_rate": 7.848708055652989e-06,
      "loss": 0.4829,
      "step": 10400
    },
    {
      "epoch": 1.8326902841108383,
      "grad_norm": 16.558080673217773,
      "learning_rate": 7.790249035426166e-06,
      "loss": 0.4626,
      "step": 10450
    },
    {
      "epoch": 1.8414591371448614,
      "grad_norm": 6.549221515655518,
      "learning_rate": 7.731790015199346e-06,
      "loss": 0.4479,
      "step": 10500
    },
    {
      "epoch": 1.8502279901788845,
      "grad_norm": 9.090127944946289,
      "learning_rate": 7.673330994972525e-06,
      "loss": 0.4742,
      "step": 10550
    },
    {
      "epoch": 1.8589968432129078,
      "grad_norm": 13.27945613861084,
      "learning_rate": 7.614871974745703e-06,
      "loss": 0.4082,
      "step": 10600
    },
    {
      "epoch": 1.867765696246931,
      "grad_norm": 15.803004264831543,
      "learning_rate": 7.556412954518883e-06,
      "loss": 0.5314,
      "step": 10650
    },
    {
      "epoch": 1.876534549280954,
      "grad_norm": 10.246562004089355,
      "learning_rate": 7.497953934292061e-06,
      "loss": 0.5112,
      "step": 10700
    },
    {
      "epoch": 1.8853034023149773,
      "grad_norm": 12.728169441223145,
      "learning_rate": 7.439494914065241e-06,
      "loss": 0.5138,
      "step": 10750
    },
    {
      "epoch": 1.8940722553490004,
      "grad_norm": 8.36591911315918,
      "learning_rate": 7.381035893838419e-06,
      "loss": 0.4772,
      "step": 10800
    },
    {
      "epoch": 1.9028411083830234,
      "grad_norm": 12.449237823486328,
      "learning_rate": 7.322576873611599e-06,
      "loss": 0.4483,
      "step": 10850
    },
    {
      "epoch": 1.9116099614170468,
      "grad_norm": 13.708969116210938,
      "learning_rate": 7.264117853384778e-06,
      "loss": 0.5059,
      "step": 10900
    },
    {
      "epoch": 1.9203788144510698,
      "grad_norm": 17.763330459594727,
      "learning_rate": 7.205658833157957e-06,
      "loss": 0.4643,
      "step": 10950
    },
    {
      "epoch": 1.929147667485093,
      "grad_norm": 13.150432586669922,
      "learning_rate": 7.147199812931136e-06,
      "loss": 0.5046,
      "step": 11000
    },
    {
      "epoch": 1.9379165205191162,
      "grad_norm": 15.133369445800781,
      "learning_rate": 7.088740792704315e-06,
      "loss": 0.4693,
      "step": 11050
    },
    {
      "epoch": 1.946685373553139,
      "grad_norm": 13.052711486816406,
      "learning_rate": 7.030281772477494e-06,
      "loss": 0.4767,
      "step": 11100
    },
    {
      "epoch": 1.9554542265871624,
      "grad_norm": 19.558963775634766,
      "learning_rate": 6.971822752250673e-06,
      "loss": 0.5093,
      "step": 11150
    },
    {
      "epoch": 1.9642230796211857,
      "grad_norm": 9.343500137329102,
      "learning_rate": 6.913363732023852e-06,
      "loss": 0.4803,
      "step": 11200
    },
    {
      "epoch": 1.9729919326552086,
      "grad_norm": 19.72092056274414,
      "learning_rate": 6.854904711797031e-06,
      "loss": 0.5526,
      "step": 11250
    },
    {
      "epoch": 1.9817607856892319,
      "grad_norm": 11.944770812988281,
      "learning_rate": 6.79644569157021e-06,
      "loss": 0.5035,
      "step": 11300
    },
    {
      "epoch": 1.990529638723255,
      "grad_norm": 13.482226371765137,
      "learning_rate": 6.737986671343388e-06,
      "loss": 0.5155,
      "step": 11350
    },
    {
      "epoch": 1.999298491757278,
      "grad_norm": 10.458133697509766,
      "learning_rate": 6.679527651116568e-06,
      "loss": 0.5046,
      "step": 11400
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.6528412877825597,
      "eval_loss": 0.8335316181182861,
      "eval_runtime": 160.7148,
      "eval_samples_per_second": 76.434,
      "eval_steps_per_second": 9.557,
      "step": 11404
    }
  ],
  "logging_steps": 50,
  "max_steps": 17106,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 996516972533142.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
